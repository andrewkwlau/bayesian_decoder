{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../library'))\n",
    "import data as d\n",
    "import preprocess as p\n",
    "import utils as u\n",
    "import bayes as b\n",
    "import pipeline\n",
    "import results as r\n",
    "import figures as figs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load data with ``mouse_ID`` and ``tau`` the time bin size of data.\n",
    "\n",
    "2. Preprocess data, output includes:\n",
    "    - ``spikeprob``\n",
    "    - ``spikes``\n",
    "    - ``position_mtx``\n",
    "    - ``darktrials``\n",
    "    - ``deltrials``\n",
    "    - ``spikeprob_shuffled``\n",
    "    - ``spikes_shuffled``\n",
    "\n",
    "3. Create an instance of the ``MouseData`` class with ``mouse_ID`` and preprocessed output. Data structure will be printed.\n",
    "\n",
    "4. Set ``tau`` and ``rewardzone`` for the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_ID = 'C57_60_Octavius'\n",
    "Octavius_200 = d.MouseData(mouse_ID, *p.preprocess_data(d.load_data(mouse_ID, tau=0.2)))\n",
    "Octavius_200.tau = 0.2 # size of time bin of the data in seconds\n",
    "Octavius_200.rewardzone = [46,47,48,49,50,51,52,53,54,55,56,57,58,59] # position bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Firing Rates Spatial Tuning Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The pipeline does the following for both ``spikes`` and ``spikeprob``:\n",
    "    - Masking data and ``position_mtx``\n",
    "    - Get trial length inforamtion for the data\n",
    "    - Gaussian smooth data\n",
    "    - Position binning data and generate tuning curves / firing rates matrix.\n",
    "    - Split data into light and dark trials.\n",
    "    - Scale data by a coefficient (only ``spikes``/firing rates, not for ``spikeprob``).\n",
    "\n",
    "2. Plot tuning curves and heatmap of a specific neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_tuning_curves(Octavius_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = Octavius_200.spikes.shape[2]\n",
    "n = np.random.randint(0, num_neurons)\n",
    "figs.plot_single_tuning(Octavius_200, Octavius_200.fr_smoothed, 'spikes', neuron_idx=n)\n",
    "figs.plot_single_heatmap(Octavius_200, Octavius_200.fr_smoothed, 'spikes', neuron_idx=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayesian Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works by first sorting the data by trial start location and separate them into 10 chunks. Each chunk with trials that have very close start locations.\n",
    "\n",
    "Then the decoder runs through each chunk and compute for each train/test paradigm:\n",
    "- ``lgtlgt``, train with firing rates in light and test on spikes in light\n",
    "- ``drkdrk``, train with firing rates in dark and test on spikes in dark\n",
    "- ``lgtdrk``, train with firing rates in light and test on spikes in dark\n",
    "- ``drklgt``, train with firing rates in dark and test on spikes in light\n",
    "\n",
    "Accuracy and Errors are computed and confusion matrices are generated. Plots are shown in Section 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Octavius_200.posterior_allchunks, Octavius_200.decoded_pos_allchunks = pipeline.run_decoder_chunks(Octavius_200, smoothfactor=0.2)\n",
    "Octavius_200.results_allchunks = pipeline.run_results_chunks(Octavius_200, Octavius_200.posterior_allchunks, Octavius_200.decoded_pos_allchunks, 46, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chance Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``spikes_shuffled`` dataset contains ``spikes`` data that are shuffled for 100 times,  breaking the spatial patterns in the data.\n",
    "\n",
    "Chance estimates are computed by running the decoder on these shuffled data.\n",
    "\n",
    "Since it will take hours to run this pipeline, the output has already been stored in the directory ``variables/``. You can load the output with ``pickle`` and run ``pipeline.run_results_chance()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_allreps, decoded_pos_allreps = pipeline.run_decoder_chance(Octavius_200,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../variables/Octavius_200_posterior_allreps.pkl', 'wb') as f:\n",
    "    pickle.dump(posterior_allreps, f)\n",
    "\n",
    "with open('../variables/Octavius_200_decoded_pos_allreps.pkl', 'wb') as f:\n",
    "    pickle.dump(decoded_pos_allreps, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../variables/Octavius_200_posterior_allreps.pkl', 'rb') as f:\n",
    "    posterior_allreps = pickle.load(f)\n",
    "\n",
    "with open('../variables/Octavius_200_decoded_pos_allreps.pkl', 'rb') as f:\n",
    "    decoded_pos_allreps = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chance_results = pipeline.run_results_chance(Octavius_200, posterior_allreps, decoded_pos_allreps, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decoder Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot confusion matrix from the decoder.\n",
    "\n",
    "Compare accuracy and errors between decoder and chance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Octavius_200.results_allchunks['mean_accuracy'])\n",
    "print(Octavius_200.results_allchunks['mean_error'])\n",
    "\n",
    "figs.plot_confusion_mtx(Octavius_200, Octavius_200.results_allchunks['confusion_mtx']['lgtlgt'], 'lgtlgt')\n",
    "figs.plot_confusion_mtx(Octavius_200, Octavius_200.results_allchunks['confusion_mtx']['drkdrk'], 'drkdrk')\n",
    "figs.plot_confusion_mtx(Octavius_200, Octavius_200.results_allchunks['confusion_mtx']['lgtdrk'], 'lgtdrk')\n",
    "figs.plot_confusion_mtx(Octavius_200, Octavius_200.results_allchunks['confusion_mtx']['drklgt'], 'drklgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs.plot_accuracy(Octavius_200, Octavius_200.results_allchunks, chance_results, 100, save=True)\n",
    "figs.plot_errors(Octavius_200, Octavius_200.results_allchunks, chance_results, 100, save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
